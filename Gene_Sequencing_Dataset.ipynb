{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gene Sequencing Dataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO/wnLS4SwgzKmafaMvUlJJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DockingBlade/Gene-Sequencing-/blob/main/Gene_Sequencing_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAO-P5eYURkw"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import sklearn\r\n",
        "from sklearn import preprocessing as sklpp\r\n",
        "from sklearn import decomposition as skldecomp \r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.neighbors import KNeighborsRegressor\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\r\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0emE-GFakw6i"
      },
      "source": [
        "df = pd.read_csv('splice.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujRACAYclP84",
        "outputId": "6045fd78-ab66-4ade-9063-8246281eca36"
      },
      "source": [
        "numDataSamples = df.shape[0];\r\n",
        "numAttributes = df.shape[1];\r\n",
        "\r\n",
        "print('The number of Data Samples is ', numDataSamples);\r\n",
        "print('The number of Attributes is ', numAttributes, ' but the Donor variable is not relevant to classification, since it is an identifier for the sample and does not have information relevant to the sample so there are really ', numAttributes-1);\r\n",
        "print( 'attributes with 1 attribute as the Label or Dependent variable and ', numAttributes-2, ' independent variables, in this case nucleotides');\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#Checing that all the nucleotide values are valid\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of Data Samples is  3190\n",
            "The number of Attributes is  62  but the Donor variable is not relevant to classification, since it is an identifier for the sample and does not have information relevant to the sample so there are really  61\n",
            "attributes with 1 attribute as the Label or Dependent variable and  60  independent variables, in this case nucleotides\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvOYolOjfFRH"
      },
      "source": [
        "Exploration and Pre-processing: This Data set consists of sequences of 60 nucleotides, classified as introns, exons, or neither with a donor number. There are 3190 samples, with 765 being introns, and 762 being exons, and 1648 being neither. The nucleotides in the sequences are Adenine (A), Cytosine (C), Guanine (G), and Thymine(T). Since all variables are categorically, I will have to convert them to numeric values. I will use  label encoding to encode the nucleotides, and the labels. The dataset also contains some sequences that have ambigious nucleotide values, so I will remove this since I don't feel like they is an appropriate way to replace them. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHVG0c0uHKvE",
        "outputId": "00f745ce-6141-4dce-a6ee-c69ea90e4ba3"
      },
      "source": [
        "#Pre-Processing\r\n",
        "\r\n",
        "#Checing that all the nucleotide values are valid\r\n",
        "for col in range(1,61):\r\n",
        "    index = str(col);\r\n",
        "    df.loc[((df[index] != 'A') & (df[index] != 'C') & (df[index] != 'G') & (df[index] != 'T')), index]= pd.NA\r\n",
        "\r\n",
        "if df.isnull().values.any():\r\n",
        "    print('Dataframe contains invalid values in the nucleotide sequences')\r\n",
        "else:\r\n",
        "    print('All entries in the nucleotide sequences are valid.')\r\n",
        "\r\n",
        "df = df.dropna();\r\n",
        "\r\n",
        "print('So in the dataset set description it mentions that nucleotides with abigious values were given different character values than A,G,C, and T. Thus I decided to remove these rows from the dataset, and now there are ',df.shape[0] , ' samples remaining');\r\n",
        "\r\n",
        "index = 'Label';\r\n",
        "df.loc[((df[index] != 'IE') & (df[index] != 'EI') & (df[index] != 'N') ), index]= pd.NA\r\n",
        "\r\n",
        "if df.isnull().values.any():\r\n",
        "    print('Dataframe contains invalid values in the Labels')\r\n",
        "else:\r\n",
        "    print('All entries in the Labels are now valid.')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#Label encoding the nucleotides\r\n",
        "for col in range(1,61):\r\n",
        "    index = str(col);\r\n",
        "    df[index] = df[index].replace(['A','C','G','T'],[0, 1, 2, 3]);\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "df['Label'] = df['Label'].replace(['IE','EI','N'],[0, 1, 2]);\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataframe contains invalid values in the nucleotide sequences\n",
            "So in the dataset set description it mentions that nucleotides with abigious values were given different character values than A,G,C, and T. Thus I decided to remove these rows from the dataset, and now there are  3175  samples remaining\n",
            "All entries in the Labels are now valid.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19Kh4N2n3m14"
      },
      "source": [
        "Only 15 samples have ambigious values in them, so I have removed them, and with the large number of samples removing 15 samples won't affect our training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuKv8DY4u63v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee59a18f-93b0-48b1-a316-73a51c0c2f90"
      },
      "source": [
        "#Feature Learning - PCA\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "nucleotides = df.iloc[:,np.r_[2:62]].to_numpy();\r\n",
        "labels = df.iloc[:,np.r_[0]].to_numpy();\r\n",
        "\r\n",
        "mean_datascaler = sklpp.StandardScaler(with_mean=True, with_std=False);\r\n",
        "data_pca = skldecomp.PCA(n_components=0.95, svd_solver='full');\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(nucleotides, labels, test_size=0.20, shuffle=True);\r\n",
        "\r\n",
        "X_train = mean_datascaler.fit_transform(X_train);\r\n",
        "X_train =  data_pca.fit_transform(X_train);\r\n",
        "mean = np.array(mean_datascaler.mean_);\r\n",
        "\r\n",
        "test_X = X_test.astype(np.float64, copy=False)\r\n",
        "for i in range(np.shape(test_X)[0]):\r\n",
        "  test_X[i,:] -= mean;\r\n",
        "\r\n",
        "U = np.transpose(data_pca.components_);\r\n",
        "U_transpose = np.transpose(U);\r\n",
        "Xtest_transpose = U_transpose @ np.transpose(test_X);\r\n",
        "X_test = np.transpose(Xtest_transpose);\r\n",
        "\r\n",
        "print('Shape of X_train,\\n', X_train.shape);\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_train,\n",
            " (2540, 56)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AW9iD8J56jj"
      },
      "source": [
        "Feature Learning :\r\n",
        "Here I do PCA, to reduce the dimensonality of the data. I set the minimum amount of energy to capture to 95%, and it reduced the number of attribute from 60 to 56. I also split the data into training and test data, with 80% into training, and 20% test. I did PCA on the training data, extracted the mean, and then I took subtracted the mean from each row in the test data, and used the components calculated by PCA to project the test data to the lower dimension. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6bezcdhIjg3"
      },
      "source": [
        "def get_score (model, train_input, train_label, test_input, test_label):\r\n",
        "  \r\n",
        "  model.fit(train_input,np.ravel(train_label));\r\n",
        "  y_pred = model.predict(test_input);\r\n",
        "  labels = np.ravel(test_label);\r\n",
        "  \r\n",
        "  return accuracy_score(labels,np.transpose(y_pred));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-eGqQKNqxsD",
        "outputId": "d18c0252-ed0d-4741-aef8-0c399bfa3a01"
      },
      "source": [
        "skf = StratifiedKFold(n_splits=5);\r\n",
        "skf.get_n_splits(X_train, y_train);\r\n",
        "k = 1;\r\n",
        "final = 1;\r\n",
        "max = 0;\r\n",
        "\r\n",
        "\r\n",
        "for train_index, test_index in skf.split(X_train, y_train):\r\n",
        "  train_input, test_input = X_train[train_index], X_train[test_index]\r\n",
        "  train_label, test_label = y_train[train_index], y_train[test_index]\r\n",
        "  knn = KNeighborsClassifier(n_neighbors=k)\r\n",
        "  val = get_score(knn, train_input, train_label, test_input, test_label)\r\n",
        "  print('for k = ', k, ' the acurracy score on the validation set is ', val );\r\n",
        "  if val > max:\r\n",
        "    max = val\r\n",
        "    final = k\r\n",
        "  k+=2\r\n",
        "\r\n",
        "print('Based on the acurracy scores on the validation sets we will choose', final, ' for the k parameter in KNN');\r\n",
        "\r\n",
        "knn = KNeighborsClassifier(n_neighbors=final)\r\n",
        "knn.fit(X_train, np.ravel(y_train));\r\n",
        "\r\n",
        "y_pred = knn.predict(test_input);\r\n",
        "labels = np.ravel(test_label);\r\n",
        "  \r\n",
        "print()\r\n",
        "Recall = recall_score(labels,np.transpose(y_pred),average= 'micro');\r\n",
        "print(\"Recall for KNN\",\"\\n\", Recall );\r\n",
        "\r\n",
        "Precision = precision_score(labels,np.transpose(y_pred),average='micro');\r\n",
        "print(\"Precision for KNN\",\"\\n\", Precision );\r\n",
        "\r\n",
        "F_score = (2 * Recall * Precision)/(Recall + Precision);\r\n",
        "print( \"F score for KNN\",\"\\n\", F_score);\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "print('The final accuracy is ',accuracy_score(labels,np.transpose(y_pred)));\r\n",
        "print('The confusion matrix is \\n', confusion_matrix(labels,np.transpose(y_pred), normalize='true'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for k =  1  the acurracy score on the validation set is  0.6318897637795275\n",
            "for k =  3  the acurracy score on the validation set is  0.6712598425196851\n",
            "for k =  5  the acurracy score on the validation set is  0.6299212598425197\n",
            "for k =  7  the acurracy score on the validation set is  0.6653543307086615\n",
            "for k =  9  the acurracy score on the validation set is  0.6633858267716536\n",
            "Based on the acurracy scores on the validation sets we will choose 3  for the k parameter in KNN\n",
            "\n",
            "Recall for KNN \n",
            " 0.8267716535433071\n",
            "Precision for KNN \n",
            " 0.8267716535433071\n",
            "F score for KNN \n",
            " 0.8267716535433071\n",
            "The final accuracy is  0.8267716535433071\n",
            "The confusion matrix is \n",
            " [[0.86885246 0.02459016 0.10655738]\n",
            " [0.07317073 0.86178862 0.06504065]\n",
            " [0.12547529 0.08365019 0.79087452]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARCvIYnf8NQS"
      },
      "source": [
        "The first method we are using is KNN. I used cross-validation to determine which value of K to use. The algorithm determined k = 9 to be the best value, (at least in this run). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q53A-IVvP6-n",
        "outputId": "da33b6f8-de7a-4bb0-d579-7e23b66c4ece"
      },
      "source": [
        "#logistic regression\r\n",
        "\r\n",
        "logRegNoPenalty = LogisticRegression(penalty='none', solver='sag', max_iter= 1000, multi_class='ovr');\r\n",
        "logRegL1 = LogisticRegression(penalty='l1', solver='saga', max_iter=1000, multi_class='ovr');\r\n",
        "logRegL2 = LogisticRegression(penalty='l2', solver='sag', max_iter=1000, multi_class='ovr');\r\n",
        "\r\n",
        "skf = StratifiedKFold(n_splits=9);\r\n",
        "skf.get_n_splits(X_train, y_train);\r\n",
        "k = 0.1;\r\n",
        "final = 0.1;\r\n",
        "max = 0;\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "print();\r\n",
        "for train_index, test_index in skf.split(X_train, y_train):\r\n",
        "  train_input, test_input = X_train[train_index], X_train[test_index]\r\n",
        "  train_label, test_label = y_train[train_index], y_train[test_index]\r\n",
        "  logRegElasticNet = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=k, max_iter=1000,multi_class='ovr');\r\n",
        "  val = get_score(logRegElasticNet, train_input, train_label, test_input, test_label)\r\n",
        "  print('for lamda = ', k, ' the acurracy score on the validation set is ', val );\r\n",
        "  if val > max:\r\n",
        "    max = val\r\n",
        "    final = k\r\n",
        "  k+=0.1\r\n",
        "print();\r\n",
        "print('Based on the acurracy scores on the validation sets we will choose', final, ' for the lamda parameter in Elasticnet ');\r\n",
        "print();\r\n",
        "\r\n",
        "logRegNoPenalty.fit(X_train, np.ravel(y_train));\r\n",
        "y_pred0 = logRegNoPenalty.predict(X_test);\r\n",
        "labels = np.ravel(y_test);\r\n",
        "noPenaltyAccuracy = accuracy_score(labels,np.transpose(y_pred0));\r\n",
        "Recall0 = recall_score(labels,np.transpose(y_pred0),average= 'micro');\r\n",
        "Precision0 = precision_score(labels,np.transpose(y_pred0),average='micro');\r\n",
        "F_score0 = (2 * Recall * Precision)/(Recall + Precision);\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "logRegL1.fit(X_train, np.ravel(y_train));\r\n",
        "y_pred1 = logRegL1.predict(X_test);\r\n",
        "labels = np.ravel(y_test);\r\n",
        "l1Accuracy = accuracy_score(labels,np.transpose(y_pred1));\r\n",
        "Recall1 = recall_score(labels,np.transpose(y_pred1),average= 'micro');\r\n",
        "Precision1 = precision_score(labels,np.transpose(y_pred1),average='micro');\r\n",
        "F_score1 = (2 * Recall * Precision)/(Recall + Precision);\r\n",
        "\r\n",
        "logRegL2.fit(X_train, np.ravel(y_train));\r\n",
        "y_pred2 = logRegL2.predict(X_test);\r\n",
        "labels = np.ravel(y_test);\r\n",
        "l2Accuracy = accuracy_score(labels,np.transpose(y_pred2));\r\n",
        "Recall2 = recall_score(labels,np.transpose(y_pred2),average= 'micro');\r\n",
        "Precision2 = precision_score(labels,np.transpose(y_pred2),average='micro');\r\n",
        "F_score2 = (2 * Recall * Precision)/(Recall + Precision);\r\n",
        "\r\n",
        "logRegElasticNet = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=final, max_iter=1000, multi_class='ovr');\r\n",
        "logRegElasticNet.fit(X_train, np.ravel(y_train));\r\n",
        "y_pred3 = logRegElasticNet.predict(X_test);\r\n",
        "labels = np.ravel(y_test);\r\n",
        "elasticAccuracy = accuracy_score(labels,np.transpose(y_pred3));\r\n",
        "Recall3 = recall_score(labels,np.transpose(y_pred3),average= 'micro');\r\n",
        "Precision3 = precision_score(labels,np.transpose(y_pred3),average='micro');\r\n",
        "F_score3 = (2 * Recall * Precision)/(Recall + Precision);\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "print('No Penalty accuracy is ', noPenaltyAccuracy);\r\n",
        "print('The confusion matrix for no penalty is \\n', confusion_matrix(labels,np.transpose(y_pred0), normalize='true'))\r\n",
        "print(\"Recall for no penalty\",\"\\n\", Recall0 );\r\n",
        "print(\"Precision for no penalty\",\"\\n\", Precision0 );\r\n",
        "print( \"F_score for no penalty\",\"\\n\", F_score0);\r\n",
        "\r\n",
        "\r\n",
        "print()\r\n",
        "print('L1 Penalty accuracy is ', l1Accuracy);\r\n",
        "print('The confusion matrix for L1 penalty is \\n', confusion_matrix(labels,np.transpose(y_pred1), normalize='true'))\r\n",
        "print(\"Recall for L1\",\"\\n\", Recall1 );\r\n",
        "print(\"Precision for L1\",\"\\n\", Precision1 );\r\n",
        "print(\"F_score for L1\",\"\\n\", F_score1);\r\n",
        "\r\n",
        "print()\r\n",
        "print('L2 Penalty accuracy is ', l2Accuracy);\r\n",
        "print('The confusion matrix for L2 penalty is \\n', confusion_matrix(labels,np.transpose(y_pred2), normalize='true'))\r\n",
        "print(\"Recall for L2\",\"\\n\", Recall2 );\r\n",
        "print(\"Precision for L2\",\"\\n\", Precision2 );\r\n",
        "print( \"F_score for L2\",\"\\n\", F_score2);\r\n",
        "\r\n",
        "print()\r\n",
        "print('Elastic Penalty accuracy is ', elasticAccuracy);\r\n",
        "print('The confusion matrix for ElasticNet is \\n', confusion_matrix(labels,np.transpose(y_pred3), normalize='true'))\r\n",
        "print(\"Recall for Elastic\",\"\\n\", Recall3 );\r\n",
        "print(\"Precision for Elastic\",\"\\n\", Precision3 );\r\n",
        "print( \"F_score for Elastic\",\"\\n\", F_score3);\r\n",
        "\r\n",
        "print()\r\n",
        "\r\n",
        "if (noPenaltyAccuracy >= l1Accuracy) and (noPenaltyAccuracy >= l2Accuracy) and (noPenaltyAccuracy >= elasticAccuracy):\r\n",
        "    print('We will choose the No Penalty Logistic Regression classifier because it has higher or equal accuracy then the other options for Logistic Regression');\r\n",
        "elif (l1Accuracy >= noPenaltyAccuracy) and (l1Accuracy >= l2Accuracy) and (l1Accuracy >= elasticAccuracy):\r\n",
        "    print('We will choose the L1 Penalty Logistic Regression classifier because it has higher or equal accuracy then the other options for Logistic Regression');\r\n",
        "elif (l2Accuracy >= noPenaltyAccuracy) and (l2Accuracy >= l1Accuracy) and (l2Accuracy >= elasticAccuracy):\r\n",
        "    print('We will choose the L2 Penalty Logistic Regression classifier because it has higher or equal accuracy then the other options for Logistic Regression');\r\n",
        "elif (elasticAccuracy >= noPenaltyAccuracy) and (elasticAccuracy >= l2Accuracy) and (elasticAccuracy >= l1Accuracy):\r\n",
        "    print('We will choose the Elasticnet Penalty Logistic Regression classifier because it has higher or equal accuracy then the other options for Logistic Regression');\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "for lamda =  0.1  the acurracy score on the validation set is  0.8091872791519434\n",
            "for lamda =  0.2  the acurracy score on the validation set is  0.8021201413427562\n",
            "for lamda =  0.30000000000000004  the acurracy score on the validation set is  0.8262411347517731\n",
            "for lamda =  0.4  the acurracy score on the validation set is  0.8014184397163121\n",
            "for lamda =  0.5  the acurracy score on the validation set is  0.8156028368794326\n",
            "for lamda =  0.6  the acurracy score on the validation set is  0.8475177304964538\n",
            "for lamda =  0.7  the acurracy score on the validation set is  0.8262411347517731\n",
            "for lamda =  0.7999999999999999  the acurracy score on the validation set is  0.7730496453900709\n",
            "for lamda =  0.8999999999999999  the acurracy score on the validation set is  0.8226950354609929\n",
            "\n",
            "Based on the acurracy scores on the validation sets we will choose 0.6  for the lamda parameter in Elasticnet \n",
            "\n",
            "No Penalty accuracy is  0.8173228346456692\n",
            "The confusion matrix for no penalty is \n",
            " [[0.80645161 0.09677419 0.09677419]\n",
            " [0.06666667 0.79333333 0.14      ]\n",
            " [0.09090909 0.07575758 0.83333333]]\n",
            "Recall for no penalty \n",
            " 0.8173228346456692\n",
            "Precision for no penalty \n",
            " 0.8173228346456692\n",
            "F_score for no penalty \n",
            " 0.8267716535433071\n",
            "\n",
            "L1 Penalty accuracy is  0.8188976377952756\n",
            "The confusion matrix for L1 penalty is \n",
            " [[0.80645161 0.09032258 0.10322581]\n",
            " [0.06666667 0.79333333 0.14      ]\n",
            " [0.08787879 0.07575758 0.83636364]]\n",
            "Recall for L1 \n",
            " 0.8188976377952756\n",
            "Precision for L1 \n",
            " 0.8188976377952756\n",
            "F_score for L1 \n",
            " 0.8267716535433071\n",
            "\n",
            "L2 Penalty accuracy is  0.8173228346456692\n",
            "The confusion matrix for L2 penalty is \n",
            " [[0.80645161 0.09677419 0.09677419]\n",
            " [0.06666667 0.79333333 0.14      ]\n",
            " [0.09090909 0.07575758 0.83333333]]\n",
            "Recall for L2 \n",
            " 0.8173228346456692\n",
            "Precision for L2 \n",
            " 0.8173228346456692\n",
            "F_score for L2 \n",
            " 0.8267716535433071\n",
            "\n",
            "Elastic Penalty accuracy is  0.8188976377952756\n",
            "The confusion matrix for ElasticNet is \n",
            " [[0.81290323 0.09032258 0.09677419]\n",
            " [0.06666667 0.79333333 0.14      ]\n",
            " [0.09090909 0.07575758 0.83333333]]\n",
            "Recall for Elastic \n",
            " 0.8188976377952756\n",
            "Precision for Elastic \n",
            " 0.8188976377952756\n",
            "F_score for Elastic \n",
            " 0.8267716535433071\n",
            "\n",
            "We will choose the L1 Penalty Logistic Regression classifier because it has higher or equal accuracy then the other options for Logistic Regression\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8Cyn0bSB92_"
      },
      "source": [
        "So Logistic Regression in Sklearn has a penalty option, with options for $||w||_1$ (L1 loss) , $||w||_2$ (L2 loss), and ElasticNet loss which is $\\lambda *||w||_1$ + $(1-\\lambda) * ||w||_2$, where $w$ is the parameter used in the $p(x)$ function, where $p(x) = 1/(1+e^{-(w^tx+b)}) $. The way that each of these are implemented in sklearn follows <br>\r\n",
        "No Penality: $\\displaystyle\\min_{w,c}  C \\sum_{n=1}^{\\infty} \\log(\\exp(-y_i*(x_i^t*w + c)) + 1)$\r\n",
        "<br>\r\n",
        "L1 Penality: $\\displaystyle\\min_{w,c} \\frac{1}{2} * ||w||_1 + C \\sum_{i=1}^{n} \\log(\\exp(-y_i*(x_i^t*w + c)) + 1)$\r\n",
        "<br>\r\n",
        "L2 Penality: $\\displaystyle\\min_{w,c} \\frac{1}{2} * ||w||_2^{2} + C \\sum_{i=1}^{n} \\log(\\exp(-y_i*(x_i^t*w + c)) + 1)$\r\n",
        "<br>\r\n",
        "Elastic Penality: $\\displaystyle\\min_{w,c} \\frac{1-\\lambda}{2} * ||w||_2^{2} + \\lambda * ||w||_1 + C \\sum_{i=1}^{n} \\log(\\exp(-y_i*(x_i^t*w + c)) + 1)$\r\n",
        "\r\n",
        "Note I have chosen to do one-vs-rest (ovr) classification since there are three classes here. SKlearn is training three sperate classifiers under the hood, and outputing the one that has the least loss as the label. \r\n",
        "\r\n",
        "Note that in the optimization there is a parameter C in each formulation, but I have left it at 1, the default value, because I am using a version of Stochastic Gradient Descent as the optimizer, and the C value only scales the derivative, but doesn't change its direction, so while it is likely to change the speed of convergence (faster if C is higher, slower if C is smaller, as this respectively increases/decreases the magnitude of the derivative), it isn't likely (in my opinon) to change the final accuracy too much.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAep8XxKQ8jS",
        "outputId": "bdbbe831-d7f5-4ce8-b8f2-8c039961c4c2"
      },
      "source": [
        "#QDA\r\n",
        "\r\n",
        "QDA = QDA();\r\n",
        "QDA.fit(X_train,np.ravel(y_train));\r\n",
        "\r\n",
        "y_pred = QDA.predict(X_test);\r\n",
        "labels = np.ravel(y_test);\r\n",
        "Recall = recall_score(labels,np.transpose(y_pred),average= 'micro');\r\n",
        "Precision = precision_score(labels,np.transpose(y_pred),average='micro');\r\n",
        "F_score = (2 * Recall * Precision)/(Recall + Precision);\r\n",
        "\r\n",
        "print('The accuracy for QDA is ', accuracy_score(labels,np.transpose(y_pred)));\r\n",
        "print('The confusion matrix for QDA is \\n', confusion_matrix(labels,np.transpose(y_pred), normalize='true'))\r\n",
        "print(\"Recall for QDA\",\"\\n\", Recall );\r\n",
        "print(\"Precision for QDA\",\"\\n\", Precision);\r\n",
        "print(\"F_score for QDA\",\"\\n\", F_score);\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy for QDA is  0.8283464566929134\n",
            "The confusion matrix for QDA is \n",
            " [[0.70322581 0.06451613 0.23225806]\n",
            " [0.06       0.71333333 0.22666667]\n",
            " [0.04242424 0.01818182 0.93939394]]\n",
            "Recall for QDA \n",
            " 0.8283464566929134\n",
            "Precision for QDA \n",
            " 0.8283464566929134\n",
            "F_score for QDA \n",
            " 0.8283464566929134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU3Ar5c0l-_r"
      },
      "source": [
        "QDA has a closed form solution, so there isn't much to say here. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jufBZREropwx"
      },
      "source": [
        "Comparative analysis of the three methods on each dataset:\r\n",
        "<br>\r\n",
        "<br>\r\n",
        "\r\n",
        "\r\n",
        "KNN :<br>\r\n",
        "KNN accuracy is  0.8267716535433071 <br>\r\n",
        "The confusion matrix for KNN is <br>\r\n",
        " [0.86885246 0.02459016 0.10655738] <br>\r\n",
        " [0.07317073 0.86178862 0.06504065] <br>\r\n",
        " [0.12547529 0.08365019 0.79087452] <br>\r\n",
        "Recall for KNN <br>\r\n",
        " 0.8267716535433071 <br>\r\n",
        "Precision for KNN <br>\r\n",
        " 0.8267716535433071 <br>\r\n",
        "F score for KNN <br>\r\n",
        " 0.8267716535433071 <br>\r\n",
        "\r\n",
        "\r\n",
        "Logistic Regression:<br>\r\n",
        "Logistic Penalty accuracy is  0.8188976377952756<br>\r\n",
        "The confusion matrix for Logistic Regression penalty is <br>\r\n",
        " [0.80645161 0.09032258 0.10322581] <br>\r\n",
        " [0.06666667 0.79333333 0.14      ] <br>\r\n",
        " [0.08787879 0.07575758 0.83636364] <br>\r\n",
        "Recall for Logistic Regression <br>\r\n",
        " 0.8188976377952756 <br>\r\n",
        "Precision for Logistic Regression <br>\r\n",
        " 0.8188976377952756 <br>\r\n",
        "F_score for Logistic Regression <br>\r\n",
        " 0.8267716535433071 <br>\r\n",
        "\r\n",
        "QDA : <br>\r\n",
        "The accuracy for QDA is  0.8283464566929134\r\n",
        "<br>\r\n",
        "The confusion matrix for QDA is \r\n",
        "<br>\r\n",
        " [0.70322581 0.06451613 0.23225806]\r\n",
        " <br>\r\n",
        " [0.06       0.71333333 0.22666667]\r\n",
        " <br>\r\n",
        " [0.04242424 0.01818182 0.93939394]\r\n",
        " <br>\r\n",
        "Recall for QDA <br>\r\n",
        " 0.8283464566929134<br>\r\n",
        "Precision for QDA <br>\r\n",
        " 0.8283464566929134 <br>\r\n",
        "F_score for QDA <br>\r\n",
        " 0.8283464566929134 <br>\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "The time complexity of KNN in training is $O(1)$, while predicting is $O(n\\log(k))$ However K is small so we can consider the prediction to be $O(n)$ time wise. \r\n",
        "\r\n",
        "The time complexity of training Logisitic Regression is $O((p+1)cnE)$ : \r\n",
        "\r\n",
        "p - number of features (+1 because of bias). Multiplication of each feature times it's weight (p operations, +1 for bias). Another p + 1 operations for summing all of them (obtaining prediction). Using gradient method to improve weights counts for the same number of operations, so in total we get 4* (p+1) (two for forward pass, two for backward), which is simply O(p+1).\r\n",
        "<br>\r\n",
        "c - number of classes (possible outputs) in your logistic regression.\r\n",
        "<br>\r\n",
        "n - number of samples in your dataset.\r\n",
        "<br>\r\n",
        "E - number of epochs you are willing to run the gradient descent (whole passes through dataset)\r\n",
        "\r\n",
        "while the time complexity of predicting with Logisitic regression is $O((p+1)c)$\r\n",
        "\r\n",
        "The time complexity of training QDA is at least $O(n*p^2)$ because of calculating the Covariance matrix \r\n",
        "\r\n",
        "The time complexity of predicting with QDA is either $O(p^2)$ or $O(p^3)$, depending on if the $det(Covariance)$ is pre-computed in the training \r\n",
        "\r\n",
        "QDA and KNN are very close in performance, but QDA has uneven performance. QDA has 0.70322581 accuracy on one class, and 0.71333333 on another class, and 0.93939394 on the last class. KNN has about 0.86, 0.86, and 0.79 on the three classes. Logistic Regression has accuracy of about 0.80, 0.79, and 0.84 on the classes, so its more even then the other two models. The three classifiers have similar Recall, and Precison, which means given a class, they are nearly equally likely to not label a mislabel an element from that class as one of the other two class (Recall), and equally likely to make a correct positive decision(Precision).\r\n",
        "\r\n",
        "\r\n",
        "From an accuracy stand point, I believe KNN performs the best since it outperforms the other two classifiers on two of the three classes on a class-by-class basis (by a .1 margin on QDA), and outperforms Logisitc Regression on accuracy, while being 0.002 of QDA in terms of acccuracy.Thus my recommendation is to use KNN. However, assuming $det(Covariance)$ is pre-computed during training for QDA, KNN has the largest prediction time in terms of complexity. Thus, if time complexity was a concern (such as n very large), I would recomend logistic regression, since its total accuracy isn't too far from the other two's total accuracy, and it has the fasted time complexity (however in this case KNN computes quickly and thus has the advantage). \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL0CfwzZE-y8"
      },
      "source": [
        "Ethics:\r\n",
        "<br>\r\n",
        "I don't think that the ability to classify introns, and exons in itself. But the accuracy that can be acheived on this problem, indicates that Machine Learning can learn features from DNA. This implies it is possible to try to map DNA sequences to traits presented in people. This information could help doctors predict potentially diseases infants can have at birth. However insurance companies, might use this information to deny or increase the cost of services, adding to the already exorbitant cost of health services. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-roCcIsME8l"
      },
      "source": [
        "Bibliography:\r\n",
        "<br>\r\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\r\n",
        "<br>\r\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\r\n",
        "<br>\r\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\r\n",
        "<br>\r\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\r\n",
        "<br>\r\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html\r\n",
        "<br>\r\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\r\n",
        "<br>\r\n",
        "https://stackoverflow.com/questions/54238493/what-is-the-search-prediction-time-complexity-of-logistic-regression\r\n",
        "<br>\r\n",
        "https://medium.com/nerd-for-tech/why-a-time-complexity-for-training-a-k-nearest-neighbors-is-o-1-8a6041c50c7f\r\n",
        "<br>\r\n",
        "http://archive.ics.uci.edu/ml/datasets/Molecular+Biology+%28Splice-junction+Gene+Sequences%29"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRqzkQawfC0Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}